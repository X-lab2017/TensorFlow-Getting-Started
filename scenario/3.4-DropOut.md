# 退出
在最后一步中，我们创建了两个卷积层并将结果展平。 现在是时候将它连接到密集的隐藏层。 这和我们在本教程的第二步中看到的类似，区别在于我们这次使用pool2_flat而不是图像作为输入。
```
W_h = tf.Variable(tf.truncated_normal([7 * 7 * 64, hidden_size], stddev=0.1))
b_h = tf.Variable(tf.constant(0.1, shape=[hidden_size]))
```
```
hidden = tf.nn.relu(tf.matmul(pool2_flat, W_h) + b_h)
```
我们现在可以将隐藏的密集层连接到输出，但我们想再做一件事 - 应用dropout。 Dropout是一种用于在训练网络时不使用某些神经元来避免过度拟合的技术。

![](http://kfcoding-static.oss-cn-hangzhou.aliyuncs.com/gitcourse-TensorFlow_getting_started/dropout.png)

仅在训练阶段dropout神经元非常重要，但在评估模型时则不然。 这就是定义额外占位符以保持dropout概率的原因。 然后我们使用tf.nn.dropout函数。
```
keep_prob = tf.placeholder(tf.float32)
hidden_drop = tf.nn.dropout(hidden, keep_prob)
```
最后一步是将其连接到输出层。
```
W = tf.Variable(tf.truncated_normal([hidden_size, labels_size], stddev=0.1))
b = tf.Variable(tf.constant(0.1, shape=[labels_size]))
```
```
output = tf.matmul(hidden_drop, W) + b
```
您可以使用以下命令运行代码：
```
python convolutional.py
```
请注意，网络的复杂性会影响训练的速度，但也提高了准确性。 可以尝试通过更改训练参数来调整结果。



