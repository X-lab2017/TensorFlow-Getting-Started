# 卷积层
我们接下来要添加的两个层是卷积网络的组成部分。 它们的工作方式与密集层的工作方式不同，并且在输入2维和更多维度的数据时效果特别好，卷积层的参数是卷积窗口和步幅的大小。 将padding设置为“SAME”表示生成的图层具有相同的大小。 完成此步骤后，我们应用最大池。 我们将构建两个卷积层，并将其连接到密集的隐藏层。 生成的体系结构可以可视化为下图：</br>
![](http://kfcoding-static.oss-cn-hangzhou.aliyuncs.com/gitcourse-TensorFlow_getting_started/convolution.png)</br>
整个网络的代码可以在convolutional.py中找到，我们现在将引导您完成它。 在前面的示例中，占位符表示展平的数字的图像和相应的标签。 虽然卷积层可以处理更高维数据，但我们需要reshape图像。
```
training_data = tf.placeholder(tf.float32, [None, image_size*image_size])
training_images = tf.reshape(training_data, [-1, image_size, image_size, 1])
labels = tf.placeholder(tf.float32, [None, labels_size])
```
下一步是为第一个卷积层设置变量。 然后我们启动卷积和最大轮询阶段。 如您所见，我们使用各种tf.nn函数，如relu，conv2d或max_pool等。 该层直接从输入数据读取重新成形的图像。
```
W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))
b_conv1 = tf.Variable(tf.constant(0.1, shape=[32]))
```
```
conv1 = tf.nn.relu(tf.nn.conv2d(training_images, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)
pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
```
第二层可以类比上一层，并在下面定义。 请注意，在输入中，它采用上一步中的最大轮询结果。
```
W_conv2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))
b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))
```
```
conv2 = tf.nn.relu(tf.nn.conv2d(pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)
pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
```
剩下的最后一件事就是将它连接到下一层，这是一个密集的隐藏层。 密集层不能与卷积的维度一起使用，因此我们需要将卷积阶段的结果展平。
```
pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])
```
下一步将展示如何将其连接到隐藏的密集层。
